 # CLIP Training Configuration
# Base configuration for FinderPartner CLIP training pipeline

# Model Configuration
model_name: openai/clip-vit-base-patch32
image_size: 224
max_length: 77
temperature: 0.07

# Training Configuration
batch_size: 4           # Reduced for small dataset - will be auto-reduced if OOM occurs
epochs: 3               # Reduced for quick testing
lr: 5.0e-5
weight_decay: 0.01
warmup_steps: 10        # Reduced for small dataset
save_steps: 5           # Save more frequently for small dataset
eval_steps: 5           # Evaluate more frequently
gradient_accumulation_steps: 1
max_grad_norm: 1.0

# Data Configuration
num_workers: 4
target_memory_gb: 8.0

# Data Paths (Update these for your dataset)
train_csv: data/captions_train.csv
val_csv: data/captions_val.csv
train_images_dir: data/images/train
val_images_dir: data/images/val
output_dir: outputs

# Optional test data
test_csv: null
test_images_dir: null

# Logging Configuration
log_wandb: true
project: FinderPartner-CLIP
run_name: null  # Will be auto-generated if null
log_dir: logs

# Device Configuration
device: auto  # Options: auto, cuda, cpu
mixed_precision: true

# Evaluation Configuration
compute_retrieval_metrics: true
retrieval_k_values: [1, 2, 3]  # Reduced for small dataset
